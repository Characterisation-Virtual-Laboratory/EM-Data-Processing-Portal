#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=gerp-monash
ControlMachine=gerp-monash-login0
BackupController=gerp-monash-login1
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmctldParameters=enable_configless
SlurmUser=slurm
SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/opt/slurm/var/state
SlurmdSpoolDir=/opt/slurm/var/spool
SwitchType=switch/none
MpiDefault=pmi2
SlurmctldPidFile=/opt/slurm/var/run/slurmctld.pid
SlurmdPidFile=/opt/slurm/var/run/slurmd.pid
#ProctrackType=proctrack/linuxproc
ProctrackType=proctrack/cgroup
#PluginDir=
#FirstJobId=
ReturnToService=1
RebootProgram=/sbin/reboot
#ResumeTimeout=300
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
TaskPlugin=task/affinity,task/cgroup
#TaskPlugin=task/affinity
#TaskPlugin=task/affinity,task/cgroup
#JobSubmitPlugins=lua
OverTimeLimit=1
CompleteWait=10

#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=3000
#SlurmdTimeout=300
#InactiveLimit=0
#MinJobAge=300
KillWait=10
#Waittime=0
#
# SCHEDULING
SchedulerType="sched/backfill"
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
SelectType="select/cons_tres"
SelectTypeParameters=CR_Core_Memory
JobContainerType=job_container/tmpfs
PriorityType=priority/multifactor
#PriorityFlags=Ticket_Based
#PriorityCalcPeriod=5
#PriorityDecayHalfLife=0
#PriorityUsageResetPeriod=14-0
PriorityWeightFairshare=10000
PriorityWeightAge=10000
PriorityWeightPartition=10000
PriorityWeightJobSize=10000
PriorityMaxAge=14-0
#
# LOGGING

SlurmctldDebug=5
SlurmctldLogFile=/mnt/slurm-logs/slurmctld.log


SlurmdDebug=5
SlurmdLogFile=/var/log/slurmd.log


SlurmSchedlogLevel=5
SlurmSchedLogFile=/mnt/slurm-logs/slurmsched.log

JobCompType=jobcomp/none
#JobCompLoc=
#

Prolog=/opt/slurm/etc/slurm.prolog
Epilog=/opt/slurm/etc/slurm.epilog

PrologFlags=contain
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
JobAcctGatherFrequency=30
#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=gerp-monash-login0

AccountingStorageBackupHost=gerp-monash-login1

AccountingStorageEnforce=limits,safe
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStorageUser=
#
#GRES
GresTypes=gpu
#

HealthCheckInterval=300
HealthCheckProgram=/opt/nhc-1.4.2/sbin/nhc

DisableRootJobs=YES
MpiParams=ports=12000-12999
# COMPUTE NODES

NodeName=gerp-monash-node00 Procs=1 RealMemory=463381 CPUs=52 Boards=1 SocketsPerBoard=1 CoresPerSocket=52 ThreadsPerCore=1 Gres=gpu:3g.20gb:2,gpu:2g.10gb:4 Weight=1 State=UNKNOWN
NodeName=gerp-monash-node01 Procs=1 RealMemory=463381 CPUs=52 Boards=1 SocketsPerBoard=1 CoresPerSocket=52 ThreadsPerCore=1 Gres=gpu:3g.20gb:2,gpu:2g.10gb:4 Weight=1 State=UNKNOWN
NodeName=gerp-monash-node02 Procs=1 RealMemory=463381 CPUs=52 Boards=1 SocketsPerBoard=1 CoresPerSocket=52 ThreadsPerCore=1 Gres=gpu:3g.20gb:2,gpu:2g.10gb:4 Weight=1 State=UNKNOWN
NodeName=gerp-monash-node03 Procs=1 RealMemory=463381 CPUs=52 Boards=1 SocketsPerBoard=1 CoresPerSocket=52 ThreadsPerCore=1 Gres=gpu:3g.20gb:2,gpu:2g.10gb:4 Weight=1 State=UNKNOWN

#This will allow 4 CPUs per master, i.e. 6 masters - 4 CPUs each per node.
PartitionName=cryosparc_master Default=yes Nodes=gerp-monash-node[00-03] MaxCPUsPerNode=24 State=UP
PartitionName=cryosparc_worker Default=yes Nodes=gerp-monash-node[00-03] State=UP
